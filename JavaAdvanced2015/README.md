# Java Advanced Course   
   
## Домашнее задание 11. [Копирование файлов](JavaAdvanced2015/src/ru/ifmo/ctddev/salynskii/UIFileCopy/)   
Создайте приложение **UIFileCopy**, которое будет осуществлять копирование файлов и директорий с демонстрацией прогресса.   
Аргументы командной строки:   
```
UIFileCopy <что копировать> <куда копировать>
```
В процессе копирования в графическом интерфейсе должны отображатьcя:   
  * текущий прогресс (progress bar)
  * прошедшее время
  * ожидаемое время до окончания копирования
  * средняя скорость копирования
  * текущая скорость копирования   
  * кнопка отмены копирования
  
При нажатии кнопки отмены копирования, копирование должно быть мгновенно прекращено. После отмены на диске могут оставаться скопированные файлы.   
При выполнении задания следует обратить внимание на:   
  * Дизайн интерфейса пользователя
  * Отзывчивость интерфейса пользователя   
  * Поведение интерфейса пользователя при изменении размеров окна
  
## Домашнее задание 9. [HelloUDP](JavaAdvanced2015/src/ru/ifmo/ctddev/salynskii/HelloUDP)   
Реализуйте клиент и сервер, взаимодействующие по UDP.   
Класс **HelloUDPClient** должен отправлять запросы на сервер, принимать результаты и выводить их на консоль.   
Аргументы командной строки:   
  * имя или ip-адрес компьютера, на котором запущен сервер
  * номер порта, на который отсылать запросы
  * префикс запросов (строка)
  * число параллельных потоков запросов
  * число запросов в каждом потоке
  
Запросы должны одновременно отсылаться в указанном числе потоков. Каждый поток должен ожидать обработки своего запроса и выводить сам запрос и результат его обработки на консоль. Если запрос не был обработан, требуется послать его заного.   
Запросы должны формироваться по схеме <code><префикс запросов><номер потока>_<номер запроса в потоке></code>   
   
Класс **HelloUDPServer** должен принимать задания, отсылаемые классом **HelloUDPClient** и отвечать на них.   
Аргументы командной строки:
  * номер порта, по которому будут приниматься запросы
  * число рабочих потоков, которые будут обрабатывать запросы

Ответом на запрос должно быть <code>Hello, <текст запроса></code>   
Если сервер не успевает обрабатывать запросы, прием запросов может быть временно приостановлен.

## Домашнее задание 8. [Web Crawler](JavaAdvanced2015/src/ru/ifmo/ctddev/salynskii/WebCrawler)
Напишите класс **WebCrawler**, который будет рекурсивно обходить сайты.   
Класс WebCrawler должен иметь конструктор   
```
public WebCrawler(Downloader downloader, int downloaders, int extractors, int perHost)
```
  * *downloader* -  позволяет скачивать страницы и извлекать из них ссылки
  * *downloaders* — максимальное число одновременно загружаемых страниц
  * *extractors* — максимальное число страниц, из которых извлекаются ссылки
  * *perHost* — максимальное число страниц, одновременно загружаемых c одного хоста.   

Для опредения хоста следует использовать метод **getHost** класса **URLUtils** из тестов.
Класс **WebCrawler** должен реализовывать интерфейс **Crawler**   
```
public interface Crawler extends AutoCloseable {   
  List<String> download(String url, int depth) throws IOException;   
  
  void close();   
}
```
Метод **download** должен рекурсивно обходить страницы, начиная с указанного URL на указанную глубину и возвращать список загруженных страниц и файлов. Например, если глубина равна 1, то должна быть загружена только указанная страница. Если глубина равна 2, то указанная страница и те страницы и файлы, на которые она ссылается и так далее.   
Загрузка и обработка страниц (извлечение ссылок) должна выполняться максимально параллельно, с учетом ограничений на число одновременно загружаемых страниц (в том числе с одного хоста) и страниц, с которых загружаются ссылки.   
Для распараллеливания разрешается создать до <code>downloaders + extractors</code> вспомогательных потоков.   
Загружать и/или извлекать ссылки из одной и той же страницы запрещается.   
Метод **close** должен завершать все вспомогательные потоки.   
Для загрузки страниц должен применяться **Downloader**, передаваемый первым аргументом конструктора.   
```
public interface Downloader {
  public Document download(final String url) throws IOException;
}
```
Метод **download** загружает документ по его адресу (URL).   
Документ позволяет получить ссылки по загруженной странице:
```
public interface Document {
  List<String> extractLinks() throws IOException;
}
```
Ссылки, возвращаемые документом являются абсолютными и имеют схему *http* или *https*.
Должен быть реализован метод **main**, позволяющий запустить обход из командной строки   
Аргументы командной строки:   
```
WebCrawler url [downloads [extractors [perHost]]]
```
Для загрузки страниц требуется использовать реализацию **CachingDownloader** из тестов.
